"""
lexicon_generator.py - ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆè¯è¡¨ã€å…³é”®è¯å’Œæ­£åˆ™è§„åˆ™

åŠŸèƒ½ï¼š
1. ä»æ ‡æ³¨æ•°æ®ä¸­æŒ‰ç±»åˆ«é‡‡æ ·æ ·æœ¬
2. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆè¯è¡¨ï¼ˆporn/politics/abuseï¼‰
3. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆæ­£åˆ™è§„åˆ™
4. ä¿å­˜åˆ° configs/lexicons/ ç›®å½•
"""

import os
import sys
import json
import logging
import argparse
import yaml
import random
import asyncio
import shutil
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
import time

# æ·»åŠ  scripts ç›®å½•åˆ°è·¯å¾„ï¼ˆç”¨äºå¯¼å…¥ llm_labelerï¼‰
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from llm_labeler import (
    call_llm_backend, 
    load_config,
    call_llm_backend_batch_with_fallback_async,
    call_llm_backend_async,
    AsyncGeminiBackend
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============ Prompt æ¨¡æ¿ ============

LEXICON_GENERATION_PROMPT = """è¯·åˆ†æä»¥ä¸‹æ•æ„Ÿæ–‡æœ¬æ ·æœ¬ï¼ˆç±»åˆ«ï¼š{category}ï¼‰ï¼Œæå–å‡ºè¯¥ç±»åˆ«ç›¸å…³çš„å…³é”®è¯ã€‚

ç±»åˆ«è¯´æ˜ï¼š
- porn: è‰²æƒ…å†…å®¹ç›¸å…³
- politics: æ¶‰æ”¿å†…å®¹ç›¸å…³
- abuse: è¾±éª‚/æ”»å‡»æ€§å†…å®¹ç›¸å…³

æ ·æœ¬åˆ—è¡¨ï¼ˆJSONæ ¼å¼ï¼‰ï¼š
{texts_json}

è¦æ±‚ï¼š
1. æå–ç›´æ¥ç›¸å…³çš„æ•æ„Ÿè¯ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡ï¼‰
2. æå–ç›¸å…³çš„åŒä¹‰è¯ã€è¿‘ä¹‰è¯
3. æå–ç›¸å…³çš„å˜ä½“è¯ï¼ˆæ‹¼éŸ³ã€è°éŸ³ç­‰ï¼‰
4. æ¯ä¸ªè¯ä¸€è¡Œï¼Œåªè¾“å‡ºè¯ï¼Œä¸è¦å…¶ä»–å†…å®¹
5. å»é™¤é‡å¤è¯
6. è‡³å°‘æå– 20 ä¸ªè¯ï¼Œæœ€å¤š 500 ä¸ªè¯

è¾“å‡ºæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªè¯ï¼Œä¸è¦ç¼–å·ï¼Œä¸è¦è¯´æ˜ï¼‰ï¼š
"""

REGEX_GENERATION_PROMPT = """è¯·ä¸ºä»¥ä¸‹æ•æ„Ÿè¯ç±»åˆ«è®¾è®¡æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™ï¼Œç”¨äºåŒ¹é…å¸¸è§çš„å˜ä½“å½¢å¼ã€‚

ç±»åˆ«ï¼š{category}
ç¤ºä¾‹æ•æ„Ÿè¯ï¼ˆå‰10ä¸ªï¼‰ï¼š
{sample_words}

è¦æ±‚ï¼š
1. è®¾è®¡åŒ¹é…æ‹¼éŸ³å˜ä½“çš„æ­£åˆ™ï¼ˆå¦‚ï¼šcao -> cao|è‰|æ“ï¼‰
2. è®¾è®¡åŒ¹é…è°éŸ³å˜ä½“çš„æ­£åˆ™ï¼ˆå¦‚ï¼šfuck -> f[u*]ck|f[u*]kï¼‰
3. è®¾è®¡åŒ¹é…ç‰¹æ®Šå­—ç¬¦æ’å…¥çš„æ­£åˆ™ï¼ˆå¦‚ï¼šf*u*c*kï¼‰
4. è®¾è®¡åŒ¹é…æ•°å­—æ›¿ä»£çš„æ­£åˆ™ï¼ˆå¦‚ï¼šf0ck, f4ckï¼‰
5. æ¯ä¸ªæ­£åˆ™ä¸€è¡Œï¼Œæ ¼å¼ï¼špattern|description
6. è‡³å°‘è®¾è®¡ 15 ä¸ªæ­£åˆ™è§„åˆ™

è¾“å‡ºæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªæ­£åˆ™ï¼Œæ ¼å¼ï¼špattern|descriptionï¼‰ï¼š
"""

LEXICON_CLEANING_PROMPT = """è¯·æ¸…æ´—ä»¥ä¸‹æ•æ„Ÿè¯è¡¨ï¼ˆç±»åˆ«ï¼š{category}ï¼‰ï¼Œåªè¿”å›åº”è¯¥ä¿ç•™çš„è¯ã€‚

ç±»åˆ«è¯´æ˜ï¼š
- porn: è‰²æƒ…å†…å®¹ç›¸å…³
- politics: æ¶‰æ”¿å†…å®¹ç›¸å…³  
- abuse: è¾±éª‚/æ”»å‡»æ€§å†…å®¹ç›¸å…³

è¯è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªè¯ï¼‰ï¼š
{words_list}

æ¸…æ´—è§„åˆ™ï¼ˆåº”åˆ é™¤çš„è¯ï¼Œä¸è¦è¿”å›è¿™äº›ï¼‰ï¼š
1. å•å­—ç¬¦è¯ï¼ˆå¦‚ï¼šA, B, +, @ï¼‰
2. è¡¨æƒ…ç¬¦å·ï¼ˆå¦‚ï¼šğŸ˜‚, ğŸ˜, ğŸ”¥ï¼‰
3. æ— æ„ä¹‰çš„ç¼–å·/ä»£ç ï¼ˆå¦‚ï¼šABF-061, ALDN-290ï¼‰
4. æ˜æ˜¾æ­£å¸¸çš„æ—¥å¸¸è¯æ±‡ï¼ˆå¦‚ï¼šè¿è¡£è£™, åˆ·ç‰™, æ”»ç•¥ï¼‰
5. çº¯æ•°å­—æˆ–çº¯å­—æ¯ä¸”é•¿åº¦â‰¤2çš„è¯
6. æ ‡ç­¾ç¬¦å·å¼€å¤´çš„è¯ï¼ˆå¦‚ï¼š#tag, @userï¼‰
7. é‡å¤è¯ï¼ˆåŒä¸€è¯å‡ºç°å¤šæ¬¡ï¼‰

ä¿ç•™è§„åˆ™ï¼ˆåº”è¿”å›çš„è¯ï¼‰ï¼š
1. çœŸæ­£çš„æ•æ„Ÿè¯ï¼ˆä¸ç±»åˆ«ç›¸å…³ï¼‰
2. æ•æ„Ÿè¯çš„å˜ä½“ã€è°éŸ³ã€æ‹¼éŸ³
3. é•¿åº¦â‰¥2ä¸”æœ‰å®é™…å«ä¹‰çš„è¯

è¾“å‡ºæ ¼å¼ï¼ˆåªè¾“å‡ºè¦ä¿ç•™çš„è¯ï¼Œæ¯è¡Œä¸€ä¸ªè¯ï¼Œä¸è¦ç¼–å·ï¼Œä¸è¦è¯´æ˜ï¼Œä¸è¦JSONæ ¼å¼ï¼‰ï¼š
"""


# ============ æ ¸å¿ƒå‡½æ•° ============

def load_data_samples(
    input_path: str,
    category: str,
    samples_per_category: Optional[int] = None,
    subtype_field: str = "subtype_label",
    coarse_field: str = "coarse_label"
) -> List[Dict]:
    """
    ä»æ•°æ®æ–‡ä»¶ä¸­åŠ è½½æŒ‡å®šç±»åˆ«çš„æ ·æœ¬
    
    Args:
        input_path: è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„
        category: ç±»åˆ«åç§°ï¼ˆporn/politics/abuseï¼‰
        samples_per_category: æ¯ä¸ªç±»åˆ«é‡‡æ ·æ•°é‡ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ ·æœ¬
        subtype_field: å­æ ‡ç­¾å­—æ®µå
        coarse_field: ç²—æ ‡ç­¾å­—æ®µå
    
    Returns:
        æ ·æœ¬åˆ—è¡¨
    """
    logger.info(f"å¼€å§‹åŠ è½½ {category} ç±»åˆ«çš„æ ·æœ¬...")
    samples = []
    
    with open(input_path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            try:
                item = json.loads(line)
                # åªå¤„ç†æ•æ„Ÿæ ·æœ¬ï¼ˆcoarse_label=1ï¼‰
                if item.get(coarse_field) != 1:
                    continue
                
                # æ£€æŸ¥å­æ ‡ç­¾
                subtype_labels = item.get(subtype_field, [])
                if not isinstance(subtype_labels, list):
                    subtype_labels = []
                
                # å¦‚æœæ ·æœ¬åŒ…å«è¯¥ç±»åˆ«ï¼Œåˆ™åŠ å…¥
                if category in subtype_labels:
                    samples.append(item)
            except json.JSONDecodeError as e:
                logger.warning(f"è·³è¿‡æ— æ•ˆJSONè¡Œ: {e}")
                continue
    
    logger.info(f"æ‰¾åˆ° {len(samples)} ä¸ª {category} ç±»åˆ«æ ·æœ¬")
    
    # å¦‚æœæŒ‡å®šäº†é‡‡æ ·æ•°é‡ï¼Œåˆ™è¿›è¡Œé‡‡æ ·ï¼›å¦åˆ™ä½¿ç”¨å…¨éƒ¨æ ·æœ¬
    if samples_per_category is not None and len(samples) > samples_per_category:
        samples = random.sample(samples, samples_per_category)
        logger.info(f"éšæœºé‡‡æ · {samples_per_category} ä¸ªæ ·æœ¬")
    else:
        logger.info(f"ä½¿ç”¨å…¨éƒ¨ {len(samples)} ä¸ªæ ·æœ¬")
    
    return samples


async def generate_lexicon_from_samples_async(
    samples: List[Dict],
    category: str,
    text_field: str = "text",
    max_retries: int = 3,
    retry_delay: float = 2.5,
    request_interval: float = 2.5,
    batch_size: int = 10,
    output_path: Optional[str] = None,
    use_async: bool = True
) -> List[str]:
    """
    å¼‚æ­¥ç‰ˆæœ¬ï¼šä½¿ç”¨å¤§æ¨¡å‹ä»æ ·æœ¬ä¸­ç”Ÿæˆè¯è¡¨ï¼ˆæ‰¹é‡å¤„ç†ï¼Œæ”¯æŒåŠ¨æ€è¿½åŠ åˆ°æ–‡ä»¶ï¼‰
    æ”¯æŒæ‰¹é‡å¤±è´¥åè‡ªåŠ¨å›é€€åˆ°é€æ¡å¤„ç†
    """
    logger.info(f"å¼€å§‹ä¸º {category} ç±»åˆ«ç”Ÿæˆè¯è¡¨ï¼ˆæ ·æœ¬æ•°ï¼š{len(samples)}ï¼Œæ‰¹é‡å¤§å°ï¼š{batch_size}ï¼Œå¼‚æ­¥æ¨¡å¼ï¼š{use_async}ï¼‰...")
    
    # å‡†å¤‡æ–‡æœ¬åˆ—è¡¨
    texts = [item.get(text_field, "") for item in samples if item.get(text_field)]
    
    all_words = []
    total_batches = (len(texts) + batch_size - 1) // batch_size
    
    # å¦‚æœæä¾›è¾“å‡ºè·¯å¾„ï¼Œå‡†å¤‡åŠ¨æ€è¿½åŠ æ¨¡å¼
    existing_words = set()
    if output_path:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯»å–å·²æœ‰è¯ï¼ˆç”¨äºå»é‡ï¼‰
        if os.path.exists(output_path):
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    existing_words = {line.strip() for line in f if line.strip()}
                logger.info(f"ä»ç°æœ‰æ–‡ä»¶è¯»å– {len(existing_words)} ä¸ªå·²æœ‰è¯ï¼Œå°†ç»§ç»­è¿½åŠ æ–°è¯...")
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰æ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    
    if not use_async:
        # å›é€€åˆ°åŒæ­¥ç‰ˆæœ¬
        return generate_lexicon_from_samples(
            samples, category, text_field, max_retries, retry_delay, 
            request_interval, batch_size, output_path
        )
    
    # åˆ›å»ºå¼‚æ­¥åç«¯å®ä¾‹ï¼ˆå…±äº«ï¼Œç¡®ä¿é€Ÿç‡é™åˆ¶ç»Ÿä¸€ï¼‰
    async_backend = AsyncGeminiBackend(max_rate=30.0)
    
    # ä½¿ç”¨åˆ—è¡¨æ¥å­˜å‚¨æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœï¼ˆé¿å…é—­åŒ…é—®é¢˜ï¼‰
    batch_results = []
    
    # å¼‚æ­¥å¤„ç†å•ä¸ªæ‰¹æ¬¡
    async def process_batch_async(batch_idx: int, batch_texts: List[str], batch_num: int):
        """å¼‚æ­¥å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼Œæ”¯æŒæ‰¹é‡å¤±è´¥åå›é€€åˆ°é€æ¡å¤„ç†"""
        batch_start_time = time.time()
        
        # å‡†å¤‡æ‰¹é‡Prompt
        prompt_prep_start = time.time()
        texts_json = json.dumps(batch_texts, ensure_ascii=False, indent=2)
        batch_prompt = LEXICON_GENERATION_PROMPT.format(
            category=category,
            texts_json=texts_json
        )
        
        # å‡†å¤‡é€æ¡Promptï¼ˆç”¨äºå›é€€ï¼‰
        single_prompts = [
            LEXICON_GENERATION_PROMPT.format(
                category=category,
                texts_json=json.dumps([text], ensure_ascii=False, indent=2)
            )
            for text in batch_texts
        ]
        prompt_prep_elapsed = time.time() - prompt_prep_start
        logger.debug(f"æ‰¹æ¬¡ {batch_num} Promptå‡†å¤‡è€—æ—¶: {prompt_prep_elapsed:.3f}ç§’")
        
        # è°ƒç”¨å¼‚æ­¥æ‰¹é‡å¤„ç†ï¼ˆæ”¯æŒå¤±è´¥å›é€€ï¼‰
        api_call_start = time.time()
        batch_response, single_results = await call_llm_backend_batch_with_fallback_async(
            batch_prompt=batch_prompt,
            single_prompts=single_prompts,
            max_retries=max_retries,
            retry_delay=retry_delay,
            max_concurrent=5,  # é€æ¡å¤„ç†æ—¶çš„æœ€å¤§å¹¶å‘æ•°
            backend=async_backend
        )
        api_call_elapsed = time.time() - api_call_start
        logger.info(f"æ‰¹æ¬¡ {batch_num} APIè°ƒç”¨æ€»è€—æ—¶: {api_call_elapsed:.2f}ç§’")
        
        batch_words = []
        
        if batch_response:
            # æ‰¹é‡å¤„ç†æˆåŠŸ
            # è§£ææ‰¹é‡å“åº”
            for line in batch_response.strip().split('\n'):
                line = line.strip()
                if not line:
                    continue
                # ç§»é™¤å¯èƒ½çš„ç¼–å·ï¼ˆå¦‚ "1. word" -> "word"ï¼‰
                line = line.lstrip('0123456789. \t-')
                if line:
                    batch_words.append(line)
            
            # æ£€æŸ¥è¿”å›çš„è¯è¡¨æ•°é‡æ˜¯å¦åˆç†
            if len(batch_words) < 5:
                logger.warning(f"æ‰¹æ¬¡ {batch_num} è¿”å›çš„è¯è¡¨æ•°é‡è¿‡å°‘ï¼ˆ{len(batch_words)} ä¸ªï¼‰ï¼Œä½¿ç”¨é€æ¡å¤„ç†ç»“æœ...")
                # æ‰¹é‡å“åº”è¯å¤ªå°‘ï¼Œä½¿ç”¨é€æ¡å¤„ç†ç»“æœ
                batch_words = []  # æ¸…ç©ºï¼Œä½¿ç”¨é€æ¡å¤„ç†ç»“æœ
                for idx, response, error in single_results:
                    if error is None and response:
                        for line in response.strip().split('\n'):
                            line = line.strip()
                            if not line:
                                continue
                            line = line.lstrip('0123456789. \t-')
                            if line:
                                batch_words.append(line)
                    elif error:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯è¶…æ—¶é”™è¯¯
                        error_msg = str(error)
                        if "è¶…æ—¶" in error_msg or "Timeout" in error_msg or isinstance(error, TimeoutError):
                            logger.warning(f"æ‰¹æ¬¡ {batch_num} æ ·æœ¬ {idx} è¶…æ—¶è·³è¿‡ï¼ˆ>20ç§’ï¼‰")
                        else:
                            logger.warning(f"æ‰¹æ¬¡ {batch_num} æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {error}")
            else:
                logger.info(f"æ‰¹æ¬¡ {batch_num} æˆåŠŸç”Ÿæˆ {len(batch_words)} ä¸ªè¯")
        else:
            # æ‰¹é‡å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨é€æ¡å¤„ç†ç»“æœ
            logger.info(f"æ‰¹æ¬¡ {batch_num} æ‰¹é‡å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨é€æ¡å¤„ç†ç»“æœ")
            for idx, response, error in single_results:
                if error is None and response:
                    for line in response.strip().split('\n'):
                        line = line.strip()
                        if not line:
                            continue
                        line = line.lstrip('0123456789. \t-')
                        if line:
                            batch_words.append(line)
                elif error:
                    logger.warning(f"æ‰¹æ¬¡ {batch_num} æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {error}")
            
            if batch_words:
                logger.info(f"æ‰¹æ¬¡ {batch_num}ï¼ˆé€æ¡å¤„ç†æ¨¡å¼ï¼‰å…±ç”Ÿæˆ {len(batch_words)} ä¸ªè¯")
        
        # è¿½åŠ åˆ°æ€»è¯è¡¨
        all_words.extend(batch_words)
        
        # å¦‚æœæä¾›äº†è¾“å‡ºè·¯å¾„ï¼Œç«‹å³è¿½åŠ æ–°è¯åˆ°æ–‡ä»¶ï¼ˆå»é‡åï¼‰
        if output_path:
            new_words = [w for w in batch_words if w not in existing_words]
            if new_words:
                with open(output_path, 'a', encoding='utf-8') as f:
                    for word in new_words:
                        f.write(word + '\n')
                existing_words.update(new_words)
                logger.info(f"æ‰¹æ¬¡ {batch_num} è¿½åŠ  {len(new_words)} ä¸ªæ–°è¯åˆ°æ–‡ä»¶ï¼ˆè·³è¿‡ {len(batch_words) - len(new_words)} ä¸ªé‡å¤è¯ï¼‰")
        
        batch_results.append(batch_words)
        
        batch_total_elapsed = time.time() - batch_start_time
        logger.info(f"æ‰¹æ¬¡ {batch_num} æ€»è€—æ—¶: {batch_total_elapsed:.2f}ç§’ï¼ˆAPI: {api_call_elapsed:.2f}ç§’ï¼Œå…¶ä»–: {batch_total_elapsed - api_call_elapsed:.2f}ç§’ï¼‰")
        
        return batch_words
    
    # é™åˆ¶å¹¶å‘æ‰¹æ¬¡æ•°é‡ï¼Œé¿å…æ‰€æœ‰æ‰¹æ¬¡åŒæ—¶å¯åŠ¨å¯¼è‡´é€Ÿç‡é™åˆ¶å™¨ä¸²è¡ŒåŒ–
    max_concurrent_batches = min(5, total_batches)  # æœ€å¤šåŒæ—¶å¤„ç†5ä¸ªæ‰¹æ¬¡
    semaphore = asyncio.Semaphore(max_concurrent_batches)
    
    async def process_batch_with_limit(batch_idx: int, batch_texts: List[str], batch_num: int):
        """å¸¦å¹¶å‘é™åˆ¶çš„æ‰¹æ¬¡å¤„ç†"""
        async with semaphore:
            return await process_batch_async(batch_idx, batch_texts, batch_num)
    
    # åˆ›å»ºæ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
    tasks = []
    for batch_idx in range(0, len(texts), batch_size):
        batch_texts = texts[batch_idx:batch_idx + batch_size]
        batch_num = batch_idx // batch_size + 1
        logger.info(f"å‡†å¤‡æ‰¹æ¬¡ {batch_num}/{total_batches}ï¼ˆ{len(batch_texts)} æ¡æ ·æœ¬ï¼‰...")
        tasks.append(process_batch_with_limit(batch_idx, batch_texts, batch_num))
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡ï¼ˆå—å¹¶å‘é™åˆ¶å’Œé€Ÿç‡é™åˆ¶å™¨æ§åˆ¶ï¼‰
    total_start_time = time.time()
    logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(tasks)} ä¸ªæ‰¹æ¬¡ï¼ˆæœ€å¤šåŒæ—¶ {max_concurrent_batches} ä¸ªï¼‰...")
    results = await asyncio.gather(*tasks)
    total_elapsed = time.time() - total_start_time
    logger.info(f"æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_elapsed:.2f}ç§’ï¼Œå¹³å‡æ¯æ‰¹æ¬¡: {total_elapsed/len(tasks):.2f}ç§’")
    
    # å»é‡å¹¶æ’åº
    all_words = sorted(list(set(all_words)))
    
    # å¦‚æœä½¿ç”¨äº†åŠ¨æ€è¿½åŠ æ¨¡å¼ï¼Œé‡æ–°æ•´ç†æ–‡ä»¶ï¼ˆå»é‡ã€æ’åºï¼‰
    if output_path:
        logger.info(f"æ•´ç†æ–‡ä»¶ï¼šå»é‡å¹¶æ’åº...")
        # è¯»å–æ–‡ä»¶ä¸­æ‰€æœ‰è¯ï¼ˆåŒ…æ‹¬åˆšæ‰è¿½åŠ çš„ï¼‰
        if os.path.exists(output_path):
            with open(output_path, 'r', encoding='utf-8') as f:
                file_words = sorted(list(set(line.strip() for line in f if line.strip())))
            # é‡æ–°å†™å…¥ï¼ˆè¦†ç›–ï¼‰
            with open(output_path, 'w', encoding='utf-8') as f:
                for word in file_words:
                    f.write(word + '\n')
            logger.info(f"âœ“ æ–‡ä»¶å·²æ•´ç†ï¼Œå…± {len(file_words)} ä¸ªè¯")
            all_words = file_words
    
    logger.info(f"âœ“ {category} è¯è¡¨ç”Ÿæˆå®Œæˆï¼Œå…± {len(all_words)} ä¸ªè¯ï¼ˆæ¥è‡ª {total_batches} ä¸ªæ‰¹æ¬¡ï¼‰")
    return all_words


def generate_lexicon_from_samples(
    samples: List[Dict],
    category: str,
    text_field: str = "text",
    max_retries: int = 3,
    retry_delay: float = 2.5,
    request_interval: float = 2.5,
    batch_size: int = 10,
    output_path: Optional[str] = None,
    use_async: bool = True
) -> List[str]:
    """
    ä½¿ç”¨å¤§æ¨¡å‹ä»æ ·æœ¬ä¸­ç”Ÿæˆè¯è¡¨ï¼ˆæ‰¹é‡å¤„ç†ï¼Œæ”¯æŒåŠ¨æ€è¿½åŠ åˆ°æ–‡ä»¶ï¼‰
    
    Args:
        samples: æ ·æœ¬åˆ—è¡¨
        category: ç±»åˆ«åç§°
        text_field: æ–‡æœ¬å­—æ®µå
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        request_interval: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œç¡®ä¿ä¸è¶…è¿‡APIé™åˆ¶ï¼ˆ30æ¬¡/åˆ†é’Ÿ = 2ç§’/æ¬¡ï¼‰
        batch_size: æ‰¹é‡å¤„ç†å¤§å°ï¼ˆé»˜è®¤10æ¡ï¼‰
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™æ¯æ‰¹ç”Ÿæˆåç«‹å³è¿½åŠ åˆ°æ–‡ä»¶
    
    Returns:
        è¯è¡¨åˆ—è¡¨ï¼ˆå»é‡ã€æ’åºï¼‰
    """
    # å¦‚æœä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼Œè°ƒç”¨å¼‚æ­¥ç‰ˆæœ¬
    if use_async:
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰äº‹ä»¶å¾ªç¯
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # å¦‚æœå·²æœ‰äº‹ä»¶å¾ªç¯åœ¨è¿è¡Œï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                logger.warning("æ£€æµ‹åˆ°è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œå°è¯•ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬...")
                # åˆ›å»ºä¸€ä¸ªæ–°çš„å¼‚æ­¥ä»»åŠ¡
                import nest_asyncio
                nest_asyncio.apply()
                return asyncio.run(generate_lexicon_from_samples_async(
                    samples, category, text_field, max_retries, retry_delay,
                    request_interval, batch_size, output_path, use_async=True
                ))
            else:
                # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ asyncio.run
                return asyncio.run(generate_lexicon_from_samples_async(
                    samples, category, text_field, max_retries, retry_delay,
                    request_interval, batch_size, output_path, use_async=True
                ))
        except RuntimeError:
            # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
            return asyncio.run(generate_lexicon_from_samples_async(
                samples, category, text_field, max_retries, retry_delay,
                request_interval, batch_size, output_path, use_async=True
            ))
        except ImportError:
            # nest_asyncio æœªå®‰è£…ï¼Œå›é€€åˆ°åŒæ­¥ç‰ˆæœ¬
            logger.warning("nest_asyncio æœªå®‰è£…ï¼Œå›é€€åˆ°åŒæ­¥æ¨¡å¼ã€‚å®‰è£…å‘½ä»¤: pip install nest-asyncio")
            use_async = False
        except Exception as e:
            logger.warning(f"å¼‚æ­¥æ¨¡å¼è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥æ¨¡å¼: {e}")
            use_async = False
    
    # åŒæ­¥ç‰ˆæœ¬ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    logger.info(f"å¼€å§‹ä¸º {category} ç±»åˆ«ç”Ÿæˆè¯è¡¨ï¼ˆæ ·æœ¬æ•°ï¼š{len(samples)}ï¼Œæ‰¹é‡å¤§å°ï¼š{batch_size}ï¼‰...")
    
    # å‡†å¤‡æ–‡æœ¬åˆ—è¡¨
    texts = [item.get(text_field, "") for item in samples if item.get(text_field)]
    
    all_words = []
    total_batches = (len(texts) + batch_size - 1) // batch_size
    
    # å¦‚æœæä¾›è¾“å‡ºè·¯å¾„ï¼Œå‡†å¤‡åŠ¨æ€è¿½åŠ æ¨¡å¼
    existing_words = set()
    if output_path:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯»å–å·²æœ‰è¯ï¼ˆç”¨äºå»é‡ï¼‰
        if os.path.exists(output_path):
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    existing_words = {line.strip() for line in f if line.strip()}
                logger.info(f"ä»ç°æœ‰æ–‡ä»¶è¯»å– {len(existing_words)} ä¸ªå·²æœ‰è¯ï¼Œå°†ç»§ç»­è¿½åŠ æ–°è¯...")
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰æ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
    
    # åˆ†æ‰¹å¤„ç†
    for batch_idx in range(0, len(texts), batch_size):
        batch_texts = texts[batch_idx:batch_idx + batch_size]
        batch_num = batch_idx // batch_size + 1
        
        logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}ï¼ˆ{len(batch_texts)} æ¡æ ·æœ¬ï¼‰...")
        
        # å‡†å¤‡æ‰¹é‡Prompt
        texts_json = json.dumps(batch_texts, ensure_ascii=False, indent=2)
        prompt = LEXICON_GENERATION_PROMPT.format(
            category=category,
            texts_json=texts_json
        )
        
        # è°ƒç”¨å¤§æ¨¡å‹
        try:
            response = call_llm_backend(prompt, max_retries=max_retries, retry_delay=retry_delay)
            
            # è§£æå“åº”
            batch_words = []
            for line in response.strip().split('\n'):
                line = line.strip()
                if not line:
                    continue
                # ç§»é™¤å¯èƒ½çš„ç¼–å·ï¼ˆå¦‚ "1. word" -> "word"ï¼‰
                line = line.lstrip('0123456789. \t-')
                if line:
                    batch_words.append(line)
            
            # æ£€æŸ¥è¿”å›çš„è¯è¡¨æ•°é‡æ˜¯å¦åˆç†
            # å¦‚æœè¿”å›çš„è¯å¤ªå°‘ï¼ˆå°‘äº5ä¸ªï¼‰ï¼Œå¯èƒ½æ˜¯è§£æå¤±è´¥ï¼Œéœ€è¦å•ç‹¬å¤„ç†
            if len(batch_words) < 5:
                logger.warning(f"æ‰¹æ¬¡ {batch_num} è¿”å›çš„è¯è¡¨æ•°é‡è¿‡å°‘ï¼ˆ{len(batch_words)} ä¸ªï¼‰ï¼Œå°è¯•å•ç‹¬å¤„ç†...")
                # å•ç‹¬å¤„ç†è¿™ä¸€æ‰¹çš„æ¯æ¡æ ·æœ¬
                for text in batch_texts:
                    try:
                        single_prompt = LEXICON_GENERATION_PROMPT.format(
                            category=category,
                            texts_json=json.dumps([text], ensure_ascii=False, indent=2)
                        )
                        single_response = call_llm_backend(single_prompt, max_retries=max_retries, retry_delay=retry_delay)
                        
                        # è§£æå•æ¡å“åº”
                        for line in single_response.strip().split('\n'):
                            line = line.strip()
                            if not line:
                                continue
                            line = line.lstrip('0123456789. \t-')
                            if line:
                                batch_words.append(line)
                        
                        # å•æ¡å¤„ç†é—´éš”
                        if request_interval > 0:
                            time.sleep(request_interval)
                    except Exception as e:
                        logger.warning(f"å•ç‹¬å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {e}")
                        continue
            else:
                logger.info(f"æ‰¹æ¬¡ {batch_num} æˆåŠŸç”Ÿæˆ {len(batch_words)} ä¸ªè¯")
            
            all_words.extend(batch_words)
            
            # å¦‚æœæä¾›äº†è¾“å‡ºè·¯å¾„ï¼Œç«‹å³è¿½åŠ æ–°è¯åˆ°æ–‡ä»¶ï¼ˆå»é‡åï¼‰
            if output_path:
                new_words = [w for w in batch_words if w not in existing_words]
                if new_words:
                    with open(output_path, 'a', encoding='utf-8') as f:
                        for word in new_words:
                            f.write(word + '\n')
                    existing_words.update(new_words)
                    logger.info(f"æ‰¹æ¬¡ {batch_num} è¿½åŠ  {len(new_words)} ä¸ªæ–°è¯åˆ°æ–‡ä»¶ï¼ˆè·³è¿‡ {len(batch_words) - len(new_words)} ä¸ªé‡å¤è¯ï¼‰")
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {e}ï¼Œå°è¯•å•ç‹¬å¤„ç†...")
            # æ‰¹é‡å¤±è´¥ï¼Œå•ç‹¬å¤„ç†è¿™ä¸€æ‰¹çš„æ¯æ¡æ ·æœ¬
            batch_words_from_single = []
            for text in batch_texts:
                try:
                    single_prompt = LEXICON_GENERATION_PROMPT.format(
                        category=category,
                        texts_json=json.dumps([text], ensure_ascii=False, indent=2)
                    )
                    single_response = call_llm_backend(single_prompt, max_retries=max_retries, retry_delay=retry_delay)
                    
                    # è§£æå•æ¡å“åº”
                    for line in single_response.strip().split('\n'):
                        line = line.strip()
                        if not line:
                            continue
                        line = line.lstrip('0123456789. \t-')
                        if line:
                            batch_words_from_single.append(line)
                            all_words.append(line)
                    
                    # å•æ¡å¤„ç†é—´éš”
                    if request_interval > 0:
                        time.sleep(request_interval)
                except Exception as e2:
                    logger.warning(f"å•ç‹¬å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {e2}")
                    continue
            
            # æ‰¹é‡å¤±è´¥åå•ç‹¬å¤„ç†æ—¶ï¼Œä¹ŸåŠ¨æ€è¿½åŠ 
            if output_path and batch_words_from_single:
                new_words = [w for w in batch_words_from_single if w not in existing_words]
                if new_words:
                    with open(output_path, 'a', encoding='utf-8') as f:
                        for word in new_words:
                            f.write(word + '\n')
                    existing_words.update(new_words)
                    logger.info(f"æ‰¹æ¬¡ {batch_num}ï¼ˆå•ç‹¬å¤„ç†æ¨¡å¼ï¼‰è¿½åŠ  {len(new_words)} ä¸ªæ–°è¯åˆ°æ–‡ä»¶")
        
        # æ‰¹é‡è¯·æ±‚é—´éš”
        if request_interval > 0 and batch_idx + batch_size < len(texts):
            time.sleep(request_interval)
    
    # å»é‡å¹¶æ’åº
    all_words = sorted(list(set(all_words)))
    
    # å¦‚æœä½¿ç”¨äº†åŠ¨æ€è¿½åŠ æ¨¡å¼ï¼Œé‡æ–°æ•´ç†æ–‡ä»¶ï¼ˆå»é‡ã€æ’åºï¼‰
    if output_path:
        logger.info(f"æ•´ç†æ–‡ä»¶ï¼šå»é‡å¹¶æ’åº...")
        # è¯»å–æ–‡ä»¶ä¸­æ‰€æœ‰è¯ï¼ˆåŒ…æ‹¬åˆšæ‰è¿½åŠ çš„ï¼‰
        if os.path.exists(output_path):
            with open(output_path, 'r', encoding='utf-8') as f:
                file_words = sorted(list(set(line.strip() for line in f if line.strip())))
            # é‡æ–°å†™å…¥ï¼ˆè¦†ç›–ï¼‰
            with open(output_path, 'w', encoding='utf-8') as f:
                for word in file_words:
                    f.write(word + '\n')
            logger.info(f"âœ“ æ–‡ä»¶å·²æ•´ç†ï¼Œå…± {len(file_words)} ä¸ªè¯")
            all_words = file_words
    
    logger.info(f"âœ“ {category} è¯è¡¨ç”Ÿæˆå®Œæˆï¼Œå…± {len(all_words)} ä¸ªè¯ï¼ˆæ¥è‡ª {total_batches} ä¸ªæ‰¹æ¬¡ï¼‰")
    return all_words


async def generate_regex_patterns_async(
    words: List[str],
    category: str,
    max_retries: int = 3,
    retry_delay: float = 2.5,
    sample_words_count: int = 20,
    output_path: Optional[str] = None,
    backend: Optional[AsyncGeminiBackend] = None,
    timeout: float = 20.0
) -> List[str]:
    """
    å¼‚æ­¥ç‰ˆæœ¬ï¼šä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ­£åˆ™è§„åˆ™ï¼ˆæ”¯æŒåŠ¨æ€è¿½åŠ åˆ°æ–‡ä»¶ï¼Œå¸¦è¶…æ—¶è·³è¿‡ï¼‰
    
    Args:
        words: è¯è¡¨åˆ—è¡¨
        category: ç±»åˆ«åç§°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        sample_words_count: ç¤ºä¾‹è¯æ•°é‡
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ç”Ÿæˆåç«‹å³è¿½åŠ åˆ°æ–‡ä»¶
        backend: å¯é€‰çš„å¼‚æ­¥åç«¯å®ä¾‹
        timeout: APIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡åˆ™è·³è¿‡
    
    Returns:
        æ­£åˆ™è§„åˆ™åˆ—è¡¨ï¼ˆæ ¼å¼ï¼špattern|descriptionï¼‰
    """
    logger.info(f"å¼€å§‹ä¸º {category} ç±»åˆ«ç”Ÿæˆæ­£åˆ™è§„åˆ™ï¼ˆå¼‚æ­¥æ¨¡å¼ï¼Œè¶…æ—¶: {timeout}ç§’ï¼‰...")
    
    # éšæœºé‡‡æ ·ç¤ºä¾‹è¯ï¼Œç¡®ä¿å¤šæ ·æ€§
    if len(words) >= sample_words_count:
        sample_words = random.sample(words, sample_words_count)
        logger.info(f"éšæœºé‡‡æ · {sample_words_count} ä¸ªè¯ä½œä¸ºç¤ºä¾‹")
    else:
        sample_words = words
        logger.info(f"è¯è¡¨æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨ {len(words)} ä¸ªè¯ä½œä¸ºç¤ºä¾‹")
    
    sample_words_str = '\n'.join(sample_words)
    
    # æ„å»ºPrompt
    prompt = REGEX_GENERATION_PROMPT.format(
        category=category,
        sample_words=sample_words_str
    )
    
    # è°ƒç”¨å¤§æ¨¡å‹ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼Œå¸¦è¶…æ—¶ï¼‰
    logger.info(f"è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆ {category} æ­£åˆ™è§„åˆ™...")
    try:
        if backend is None:
            backend = AsyncGeminiBackend(max_rate=30.0)
        
        # æ·»åŠ è¶…æ—¶é™åˆ¶
        response = await asyncio.wait_for(
            call_llm_backend_async(
                prompt, 
                max_retries=max_retries, 
                retry_delay=retry_delay,
                backend=backend
            ),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        logger.warning(f"ç”Ÿæˆ {category} æ­£åˆ™è§„åˆ™è¶…æ—¶ï¼ˆ>{timeout}ç§’ï¼‰ï¼Œè·³è¿‡æœ¬æ¬¡è°ƒç”¨")
        return []
    except Exception as e:
        logger.error(f"ç”Ÿæˆ {category} æ­£åˆ™è§„åˆ™å¤±è´¥: {e}")
        return []
    
    # è§£æå“åº”
    patterns = []
    for line in response.strip().split('\n'):
        line = line.strip()
        if not line:
            continue
        # ç§»é™¤å¯èƒ½çš„ç¼–å·
        line = line.lstrip('0123456789. \t-')
        if line and '|' in line:
            patterns.append(line)
    
    # å¦‚æœæä¾›äº†è¾“å‡ºè·¯å¾„ï¼Œç«‹å³è¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå»é‡ï¼‰
    if output_path:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        existing_patterns = set()
        # è¯»å–å·²æœ‰è§„åˆ™
        if os.path.exists(output_path):
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    existing_patterns = {line.strip() for line in f if line.strip()}
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰æ­£åˆ™è§„åˆ™æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        # è¿½åŠ æ–°è§„åˆ™ï¼ˆå»é‡ï¼‰
        new_patterns = [p for p in patterns if p not in existing_patterns]
        if new_patterns:
            with open(output_path, 'a', encoding='utf-8') as f:
                for pattern in new_patterns:
                    f.write(pattern + '\n')
            logger.info(f"è¿½åŠ  {len(new_patterns)} ä¸ªæ–°æ­£åˆ™è§„åˆ™åˆ°æ–‡ä»¶ï¼ˆè·³è¿‡ {len(patterns) - len(new_patterns)} ä¸ªé‡å¤è§„åˆ™ï¼‰")
        
        # åˆå¹¶æ‰€æœ‰è§„åˆ™ï¼ˆåŒ…æ‹¬å·²æœ‰çš„ï¼‰
        patterns = list(existing_patterns) + new_patterns
    
    logger.info(f"âœ“ {category} æ­£åˆ™è§„åˆ™ç”Ÿæˆå®Œæˆï¼Œå…± {len(patterns)} ä¸ªè§„åˆ™")
    return patterns


def generate_regex_patterns(
    words: List[str],
    category: str,
    max_retries: int = 3,
    retry_delay: float = 2.5,
    request_interval: float = 2.5,
    sample_words_count: int = 20,
    output_path: Optional[str] = None,
    use_async: bool = True,
    timeout: float = 20.0
) -> List[str]:
    """
    ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ­£åˆ™è§„åˆ™ï¼ˆæ”¯æŒåŠ¨æ€è¿½åŠ åˆ°æ–‡ä»¶ï¼‰
    
    Args:
        words: è¯è¡¨åˆ—è¡¨
        category: ç±»åˆ«åç§°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        request_interval: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œç¡®ä¿ä¸è¶…è¿‡APIé™åˆ¶ï¼ˆ30æ¬¡/åˆ†é’Ÿ = 2ç§’/æ¬¡ï¼‰ï¼ˆä»…åŒæ­¥æ¨¡å¼ä½¿ç”¨ï¼‰
        sample_words_count: ç¤ºä¾‹è¯æ•°é‡
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ç”Ÿæˆåç«‹å³è¿½åŠ åˆ°æ–‡ä»¶
        use_async: æ˜¯å¦ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
        timeout: APIè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡åˆ™è·³è¿‡ï¼ˆä»…å¼‚æ­¥æ¨¡å¼ä½¿ç”¨ï¼‰
    
    Returns:
        æ­£åˆ™è§„åˆ™åˆ—è¡¨ï¼ˆæ ¼å¼ï¼špattern|descriptionï¼‰
    """
    # å¦‚æœä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼Œè°ƒç”¨å¼‚æ­¥ç‰ˆæœ¬
    if use_async:
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰äº‹ä»¶å¾ªç¯
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # å¦‚æœå·²æœ‰äº‹ä»¶å¾ªç¯åœ¨è¿è¡Œï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                logger.warning("æ£€æµ‹åˆ°è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œå°è¯•ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬...")
                # åˆ›å»ºä¸€ä¸ªæ–°çš„å¼‚æ­¥ä»»åŠ¡
                try:
                    import nest_asyncio
                    nest_asyncio.apply()
                    return asyncio.run(generate_regex_patterns_async(
                        words, category, max_retries, retry_delay,
                        sample_words_count, output_path, None, timeout
                    ))
                except ImportError:
                    logger.warning("nest_asyncio æœªå®‰è£…ï¼Œå›é€€åˆ°åŒæ­¥æ¨¡å¼ã€‚å®‰è£…å‘½ä»¤: pip install nest-asyncio")
                    use_async = False
            else:
                # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ asyncio.run
                return asyncio.run(generate_regex_patterns_async(
                    words, category, max_retries, retry_delay,
                    sample_words_count, output_path, None, timeout
                ))
        except RuntimeError:
            # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
            return asyncio.run(generate_regex_patterns_async(
                words, category, max_retries, retry_delay,
                sample_words_count, output_path, None, timeout
            ))
        except Exception as e:
            logger.warning(f"å¼‚æ­¥æ¨¡å¼è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥æ¨¡å¼: {e}")
            use_async = False
    
    # åŒæ­¥ç‰ˆæœ¬ï¼ˆå›é€€ï¼‰
    logger.info(f"å¼€å§‹ä¸º {category} ç±»åˆ«ç”Ÿæˆæ­£åˆ™è§„åˆ™ï¼ˆåŒæ­¥æ¨¡å¼ï¼‰...")
    
    # éšæœºé‡‡æ ·ç¤ºä¾‹è¯ï¼Œç¡®ä¿å¤šæ ·æ€§
    if len(words) >= sample_words_count:
        sample_words = random.sample(words, sample_words_count)
        logger.info(f"éšæœºé‡‡æ · {sample_words_count} ä¸ªè¯ä½œä¸ºç¤ºä¾‹")
    else:
        sample_words = words
        logger.info(f"è¯è¡¨æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨å…¨éƒ¨ {len(words)} ä¸ªè¯ä½œä¸ºç¤ºä¾‹")
    
    sample_words_str = '\n'.join(sample_words)
    
    # æ„å»ºPrompt
    prompt = REGEX_GENERATION_PROMPT.format(
        category=category,
        sample_words=sample_words_str
    )
    
    # è°ƒç”¨å¤§æ¨¡å‹
    logger.info(f"è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆ {category} æ­£åˆ™è§„åˆ™...")
    response = call_llm_backend(prompt, max_retries=max_retries, retry_delay=retry_delay)
    
    # æ·»åŠ è¯·æ±‚é—´éš”ï¼Œç¡®ä¿ä¸è¶…è¿‡APIé™åˆ¶ï¼ˆ30æ¬¡/åˆ†é’Ÿï¼‰
    if request_interval > 0:
        logger.debug(f"ç­‰å¾… {request_interval} ç§’åç»§ç»­ï¼ˆAPIé™åˆ¶ï¼š30æ¬¡/åˆ†é’Ÿï¼‰")
        time.sleep(request_interval)
    
    # è§£æå“åº”
    patterns = []
    for line in response.strip().split('\n'):
        line = line.strip()
        if not line:
            continue
        # ç§»é™¤å¯èƒ½çš„ç¼–å·
        line = line.lstrip('0123456789. \t-')
        if line and '|' in line:
            patterns.append(line)
    
    # å¦‚æœæä¾›äº†è¾“å‡ºè·¯å¾„ï¼Œç«‹å³è¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå»é‡ï¼‰
    if output_path:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        existing_patterns = set()
        # è¯»å–å·²æœ‰è§„åˆ™
        if os.path.exists(output_path):
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    existing_patterns = {line.strip() for line in f if line.strip()}
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰æ­£åˆ™è§„åˆ™æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        # è¿½åŠ æ–°è§„åˆ™ï¼ˆå»é‡ï¼‰
        new_patterns = [p for p in patterns if p not in existing_patterns]
        if new_patterns:
            with open(output_path, 'a', encoding='utf-8') as f:
                for pattern in new_patterns:
                    f.write(pattern + '\n')
            logger.info(f"è¿½åŠ  {len(new_patterns)} ä¸ªæ–°æ­£åˆ™è§„åˆ™åˆ°æ–‡ä»¶ï¼ˆè·³è¿‡ {len(patterns) - len(new_patterns)} ä¸ªé‡å¤è§„åˆ™ï¼‰")
        
        # åˆå¹¶æ‰€æœ‰è§„åˆ™ï¼ˆåŒ…æ‹¬å·²æœ‰çš„ï¼‰
        patterns = list(existing_patterns) + new_patterns
    
    logger.info(f"âœ“ {category} æ­£åˆ™è§„åˆ™ç”Ÿæˆå®Œæˆï¼Œå…± {len(patterns)} ä¸ªè§„åˆ™")
    return patterns


def save_lexicon_file(
    words: List[str],
    output_path: str,
    category: str
):
    """
    ä¿å­˜è¯è¡¨åˆ°æ–‡ä»¶
    
    Args:
        words: è¯è¡¨åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        category: ç±»åˆ«åç§°
    """
    logger.info(f"ä¿å­˜ {category} è¯è¡¨åˆ°: {output_path}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for word in words:
            f.write(word + '\n')
    
    logger.info(f"âœ“ {category} è¯è¡¨å·²ä¿å­˜ï¼Œå…± {len(words)} ä¸ªè¯")


def save_regex_file(
    patterns: List[str],
    output_path: str
):
    """
    ä¿å­˜æ­£åˆ™è§„åˆ™åˆ°æ–‡ä»¶
    
    Args:
        patterns: æ­£åˆ™è§„åˆ™åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    logger.info(f"ä¿å­˜æ­£åˆ™è§„åˆ™åˆ°: {output_path}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for pattern in patterns:
            f.write(pattern + '\n')
    
    logger.info(f"âœ“ æ­£åˆ™è§„åˆ™å·²ä¿å­˜ï¼Œå…± {len(patterns)} ä¸ªè§„åˆ™")


# ============ è¯è¡¨æ¸…æ´—åŠŸèƒ½ ============

async def clean_lexicon_batch_async(
    batch_words: List[str],
    category: str,
    batch_idx: int,
    total_batches: int,
    async_backend: AsyncGeminiBackend,
    max_retries: int = 3,
    retry_delay: float = 2.5
) -> List[str]:
    """
    å¼‚æ­¥æ¸…æ´—å•ä¸ªæ‰¹æ¬¡çš„è¯è¡¨
    
    Args:
        batch_words: æ‰¹æ¬¡è¯è¡¨åˆ—è¡¨
        category: ç±»åˆ«åç§°
        batch_idx: æ‰¹æ¬¡ç´¢å¼•
        total_batches: æ€»æ‰¹æ¬¡æ•°
        async_backend: å¼‚æ­¥åç«¯å®ä¾‹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿ
    
    Returns:
        æ¸…æ´—åä¿ç•™çš„è¯åˆ—è¡¨
    """
    prompt = LEXICON_CLEANING_PROMPT.format(
        category=category,
        words_list='\n'.join(batch_words)
    )
    
    cleaned_words = []
    
    for attempt in range(max_retries):
        try:
            response = await call_llm_backend_async(
                prompt,
                max_retries=1,  # è¿™é‡Œåªé‡è¯•ä¸€æ¬¡ï¼Œå¤–å±‚å¾ªç¯å¤„ç†é‡è¯•
                retry_delay=retry_delay,
                backend=async_backend
            )
            
            if response:
                # è§£æå“åº”ï¼Œæå–ä¿ç•™çš„è¯
                for line in response.strip().split('\n'):
                    line = line.strip()
                    if not line:
                        continue
                    # ç§»é™¤å¯èƒ½çš„ç¼–å·ï¼ˆå¦‚ "1. word" -> "word"ï¼‰
                    line = line.lstrip('0123456789. \t-')
                    if line and line in batch_words:  # åªä¿ç•™åŸå§‹è¯è¡¨ä¸­çš„è¯
                        cleaned_words.append(line)
                
                logger.info(f"æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} æ¸…æ´—å®Œæˆï¼Œä¿ç•™ {len(cleaned_words)}/{len(batch_words)} ä¸ªè¯")
                return cleaned_words
            else:
                logger.warning(f"æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} è¿”å›ç©ºå“åº”ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰")
        
        except Exception as e:
            logger.warning(f"æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} æ¸…æ´—å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
            else:
                logger.error(f"æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} æ¸…æ´—å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
    
    return cleaned_words


async def clean_lexicon_with_llm_async(
    words: List[str],
    category: str,
    batch_size: int = 100,
    max_retries: int = 3,
    retry_delay: float = 2.5,
    temp_file_path: Optional[str] = None
) -> Tuple[List[str], int]:
    """
    ä½¿ç”¨ LLM å¼‚æ­¥æ‰¹é‡æ¸…æ´—è¯è¡¨
    
    Args:
        words: åŸå§‹è¯è¡¨åˆ—è¡¨
        category: ç±»åˆ«åç§°
        batch_size: æ¯æ‰¹å¤„ç†çš„è¯æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿ
        temp_file_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºé€æ¬¡è¿½åŠ ç»“æœï¼‰
    
    Returns:
        (æ¸…æ´—åçš„è¯è¡¨, åˆ é™¤çš„è¯æ•°)
    """
    logger.info(f"å¼€å§‹æ¸…æ´— {category} è¯è¡¨ï¼ˆåŸå§‹è¯æ•°ï¼š{len(words)}ï¼Œæ‰¹æ¬¡å¤§å°ï¼š{batch_size}ï¼‰...")
    
    # æ£€æŸ¥API keyæ˜¯å¦è®¾ç½®
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        logger.error("=" * 60)
        logger.error("GEMINI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæ— æ³•ä½¿ç”¨LLMæ¸…æ´—")
        logger.error("=" * 60)
        logger.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡åå†è¿è¡Œï¼š")
        logger.error("  PowerShell: $env:GEMINI_API_KEY = 'your_api_key'")
        logger.error("  CMD: set GEMINI_API_KEY=your_api_key")
        logger.error("  Linux/Mac: export GEMINI_API_KEY=your_api_key")
        raise ValueError("GEMINI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    
    # è¿‡æ»¤ç©ºè¯
    filtered_words = [w.strip() for w in words if w.strip()]
    if len(filtered_words) == 0:
        logger.warning(f"è¯è¡¨ä¸ºç©ºï¼Œæ¸…æ´—å®Œæˆ")
        return [], len(words)
    
    # å‡†å¤‡ä¸´æ—¶æ–‡ä»¶
    if temp_file_path:
        os.makedirs(os.path.dirname(temp_file_path) if os.path.dirname(temp_file_path) else '.', exist_ok=True)
        # æ¸…ç©ºä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists(temp_file_path):
            open(temp_file_path, 'w', encoding='utf-8').close()
    
    # åˆ†æ‰¹å¤„ç†ï¼ˆLLMæ¸…æ´—ï¼‰
    total_batches = (len(filtered_words) + batch_size - 1) // batch_size
    logger.info(f"ä½¿ç”¨LLMæ¸…æ´—è¯è¡¨ï¼ˆå…± {total_batches} ä¸ªæ‰¹æ¬¡ï¼‰...")
    
    # åˆ›å»ºå¼‚æ­¥åç«¯å®ä¾‹ï¼ˆå…±äº«ï¼Œç¡®ä¿é€Ÿç‡é™åˆ¶ç»Ÿä¸€ï¼‰
    async_backend = AsyncGeminiBackend(max_rate=30.0)
    
    # åˆ›å»ºæ‰¹æ¬¡ä»»åŠ¡
    batch_tasks = []
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(filtered_words))
        batch_words = filtered_words[start_idx:end_idx]
        
        task = clean_lexicon_batch_async(
            batch_words,
            category,
            batch_idx,
            total_batches,
            async_backend,
            max_retries,
            retry_delay
        )
        batch_tasks.append((batch_idx, task))
    
    # å¹¶å‘æ‰§è¡Œæ‰¹æ¬¡ä»»åŠ¡ï¼ˆä½†å—é€Ÿç‡é™åˆ¶ï¼‰
    all_cleaned_words = []
    for batch_idx, task in batch_tasks:
        try:
            batch_cleaned = await task
            
            # è¿½åŠ åˆ°ä¸´æ—¶æ–‡ä»¶
            if temp_file_path and batch_cleaned:
                with open(temp_file_path, 'a', encoding='utf-8') as f:
                    for word in batch_cleaned:
                        f.write(word + '\n')
            
            all_cleaned_words.extend(batch_cleaned)
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¼‚å¸¸: {e}")
            continue
    
    # 4. ä»ä¸´æ—¶æ–‡ä»¶è¯»å–å¹¶å»é‡ï¼ˆå¦‚æœä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ï¼‰
    if temp_file_path and os.path.exists(temp_file_path):
        logger.info("æ­¥éª¤3: ä»ä¸´æ—¶æ–‡ä»¶è¯»å–å¹¶å»é‡...")
        with open(temp_file_path, 'r', encoding='utf-8') as f:
            temp_words = [line.strip() for line in f if line.strip()]
        cleaned_words = list(set(temp_words))  # å»é‡
        logger.info(f"âœ“ å»é‡å®Œæˆï¼Œæœ€ç»ˆä¿ç•™ {len(cleaned_words)} ä¸ªè¯")
    else:
        # å¦‚æœæ²¡æœ‰ä¸´æ—¶æ–‡ä»¶ï¼Œç›´æ¥å»é‡
        cleaned_words = list(set(all_cleaned_words))
    
    removed_count = len(words) - len(cleaned_words)
    removal_rate = removed_count / len(words) if len(words) > 0 else 0.0
    
    logger.info(f"âœ“ {category} è¯è¡¨æ¸…æ´—å®Œæˆ")
    logger.info(f"  - åŸå§‹è¯æ•°: {len(words)}")
    logger.info(f"  - æ¸…æ´—åè¯æ•°: {len(cleaned_words)}")
    logger.info(f"  - åˆ é™¤è¯æ•°: {removed_count}")
    logger.info(f"  - åˆ é™¤ç‡: {removal_rate:.2%}")
    
    return cleaned_words, removed_count


def clean_lexicon_with_llm(
    words: List[str],
    category: str,
    batch_size: int = 100,
    max_retries: int = 3,
    retry_delay: float = 2.5,
    temp_file_path: Optional[str] = None,
    use_async: bool = True
) -> Tuple[List[str], int]:
    """
    ä½¿ç”¨ LLM æ¸…æ´—è¯è¡¨ï¼ˆåŒæ­¥åŒ…è£…å‡½æ•°ï¼‰
    
    Args:
        words: åŸå§‹è¯è¡¨åˆ—è¡¨
        category: ç±»åˆ«åç§°
        batch_size: æ¯æ‰¹å¤„ç†çš„è¯æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿ
        temp_file_path: ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        use_async: æ˜¯å¦ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
    
    Returns:
        (æ¸…æ´—åçš„è¯è¡¨, åˆ é™¤çš„è¯æ•°)
    """
    if use_async:
        try:
            # å°è¯•ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬
            try:
                loop = asyncio.get_event_loop()
                if loop.is_closed():
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
            except RuntimeError:
                # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            if loop.is_running():
                # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œä½¿ç”¨ nest_asyncio
                try:
                    import nest_asyncio
                    nest_asyncio.apply()
                except ImportError:
                    logger.warning("äº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œä¸”æœªå®‰è£… nest_asyncioï¼Œåˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯")
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
            
            return loop.run_until_complete(
                clean_lexicon_with_llm_async(
                    words, category, batch_size, max_retries, retry_delay, temp_file_path
                )
            )
        except Exception as e:
            logger.warning(f"å¼‚æ­¥æ¨¡å¼å¤±è´¥ï¼Œå›é€€åˆ°åŒæ­¥æ¨¡å¼: {e}")
            use_async = False
    
    # åŒæ­¥æ¨¡å¼ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸æ¨èç”¨äºå¤§æ‰¹é‡ï¼‰
    logger.warning("ä½¿ç”¨åŒæ­¥æ¨¡å¼æ¸…æ´—è¯è¡¨ï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼‰")
    # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(
            clean_lexicon_with_llm_async(
                words, category, batch_size, max_retries, retry_delay, temp_file_path
            )
        )
    finally:
        loop.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆè¯è¡¨å’Œæ­£åˆ™è§„åˆ™')
    parser.add_argument(
        '--input',
        type=str,
        default='data/train.jsonl',
        help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼ŒåŒ…å«subtype_labelå­—æ®µï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='configs/lexicons',
        help='è¾“å‡ºç›®å½•ï¼ˆè¯è¡¨æ–‡ä»¶å°†ä¿å­˜åˆ°æ­¤ç›®å½•ï¼‰'
    )
    parser.add_argument(
        '--samples-per-category',
        type=int,
        default=None,
        help='æ¯ä¸ªç±»åˆ«é‡‡æ ·æ ·æœ¬æ•°ï¼ˆé»˜è®¤ï¼šNoneï¼Œä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼‰'
    )
    parser.add_argument(
        '--categories',
        type=str,
        nargs='+',
        default=['porn', 'politics', 'abuse'],
        help='è¦ç”Ÿæˆçš„ç±»åˆ«åˆ—è¡¨ï¼ˆé»˜è®¤ï¼šporn politics abuseï¼‰'
    )
    parser.add_argument(
        '--generate-regex',
        action='store_true',
        help='æ˜¯å¦ç”Ÿæˆæ­£åˆ™è§„åˆ™ï¼ˆé»˜è®¤ï¼šFalseï¼‰'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='éšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼‰'
    )
    parser.add_argument(
        '--clean-lexicon',
        action='store_true',
        help='æ¸…æ´—ç°æœ‰è¯è¡¨ï¼ˆä½¿ç”¨LLMåˆ¤æ–­ï¼Œè¦†ç›–æ‰€æœ‰è¯è¡¨ï¼‰'
    )
    parser.add_argument(
        '--lexicon-dir',
        type=str,
        default='configs/lexicons',
        help='è¯è¡¨ç›®å½•ï¼ˆæ¸…æ´—æ¨¡å¼ä½¿ç”¨ï¼Œé»˜è®¤ï¼šconfigs/lexiconsï¼‰'
    )
    parser.add_argument(
        '--clean-batch-size',
        type=int,
        default=100,
        help='æ¸…æ´—æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ï¼š100ï¼‰'
    )
    parser.add_argument(
        '--backup-original',
        action='store_true',
        help='æ¸…æ´—å‰å¤‡ä»½åŸå§‹è¯è¡¨æ–‡ä»¶ï¼ˆé»˜è®¤ï¼šFalseï¼‰'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    
    # åŠ è½½é…ç½®ï¼Œè·å–è¯·æ±‚é—´éš”
    try:
        config = load_config()
        request_interval = config.get('llm', {}).get('request_interval', 2.5)
        logger.info(f"ä»é…ç½®æ–‡ä»¶è¯»å–è¯·æ±‚é—´éš”: {request_interval} ç§’")
    except Exception as e:
        logger.warning(f"æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤è¯·æ±‚é—´éš” 2.5 ç§’: {e}")
        request_interval = 2.5
    
    # å¦‚æœæ˜¯æ¸…æ´—æ¨¡å¼ï¼Œæ‰§è¡Œæ¸…æ´—é€»è¾‘
    if args.clean_lexicon:
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ¸…æ´—è¯è¡¨ï¼ˆä½¿ç”¨LLMï¼‰")
        logger.info("=" * 60)
        logger.info(f"è¯è¡¨ç›®å½•: {args.lexicon_dir}")
        logger.info(f"æ¸…æ´—ç±»åˆ«: {args.categories}")
        logger.info(f"æ‰¹æ¬¡å¤§å°: {args.clean_batch_size}")
        logger.info(f"å¤‡ä»½åŸå§‹æ–‡ä»¶: {args.backup_original}")
        logger.info("")
        
        # ç¡®ä¿è¯è¡¨ç›®å½•å­˜åœ¨
        if not os.path.exists(args.lexicon_dir):
            logger.error(f"è¯è¡¨ç›®å½•ä¸å­˜åœ¨: {args.lexicon_dir}")
            return 1
        
        total_original = 0
        total_cleaned = 0
        total_removed = 0
        
        for category in args.categories:
            lexicon_path = os.path.join(args.lexicon_dir, f"{category}.txt")
            
            if not os.path.exists(lexicon_path):
                logger.warning(f"è¯è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {lexicon_path}ï¼Œè·³è¿‡")
                continue
            
            logger.info("")
            logger.info("-" * 60)
            logger.info(f"æ¸…æ´—ç±»åˆ«: {category}")
            logger.info("-" * 60)
            
            # å¤‡ä»½åŸå§‹æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if args.backup_original:
                backup_path = os.path.join(args.lexicon_dir, f"{category}.txt.backup")
                shutil.copy2(lexicon_path, backup_path)
                logger.info(f"âœ“ å·²å¤‡ä»½åŸå§‹æ–‡ä»¶åˆ°: {backup_path}")
            
            # è¯»å–åŸå§‹è¯è¡¨
            logger.info(f"è¯»å–è¯è¡¨: {lexicon_path}")
            original_words = []
            with open(lexicon_path, 'r', encoding='utf-8') as f:
                for line in f:
                    word = line.strip()
                    if word:
                        original_words.append(word)
            
            logger.info(f"åŸå§‹è¯æ•°: {len(original_words)}")
            
            if len(original_words) == 0:
                logger.warning(f"è¯è¡¨ä¸ºç©ºï¼Œè·³è¿‡æ¸…æ´—")
                continue
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„
            temp_file_path = os.path.join(args.lexicon_dir, f"{category}.txt.temp")
            
            # æ¸…æ´—è¯è¡¨ï¼ˆåŒ…å«LLMæ¸…æ´—ï¼‰
            try:
                cleaned_words, removed_count = clean_lexicon_with_llm(
                    original_words,
                    category,
                    batch_size=args.clean_batch_size,
                    max_retries=3,
                    retry_delay=request_interval,
                    temp_file_path=temp_file_path,
                    use_async=True
                )
                
                # ä¿å­˜æ¸…æ´—åçš„è¯è¡¨ï¼ˆè¦†ç›–åŸæ–‡ä»¶ï¼‰
                logger.info(f"ä¿å­˜æ¸…æ´—åçš„è¯è¡¨åˆ°: {lexicon_path}")
                with open(lexicon_path, 'w', encoding='utf-8') as f:
                    for word in sorted(cleaned_words):  # æ’åºåä¿å­˜
                        f.write(word + '\n')
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_file_path):
                    os.remove(temp_file_path)
                
                total_original += len(original_words)
                total_cleaned += len(cleaned_words)
                total_removed += removed_count
                
                logger.info(f"âœ“ {category} è¯è¡¨æ¸…æ´—å®Œæˆ")
                
            except Exception as e:
                logger.error(f"âœ— æ¸…æ´— {category} è¯è¡¨æ—¶å‡ºé”™: {e}", exc_info=True)
                # å¦‚æœå‡ºé”™ï¼Œåˆ é™¤ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_file_path):
                    os.remove(temp_file_path)
                continue
        
        # æ€»ç»“
        logger.info("")
        logger.info("=" * 60)
        logger.info("è¯è¡¨æ¸…æ´—å®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"æ€»åŸå§‹è¯æ•°: {total_original}")
        logger.info(f"æ€»æ¸…æ´—åè¯æ•°: {total_cleaned}")
        logger.info(f"æ€»åˆ é™¤è¯æ•°: {total_removed}")
        if total_original > 0:
            logger.info(f"æ€»åˆ é™¤ç‡: {total_removed / total_original:.2%}")
        logger.info("")
        logger.info("æç¤ºï¼šæ¸…æ´—åè¯·äººå·¥å®¡æ ¸è¯è¡¨ï¼Œç¡®ä¿æ²¡æœ‰è¯¯åˆ é‡è¦æ•æ„Ÿè¯")
        
        return 0
    
    # ç”Ÿæˆè¯è¡¨æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    logger.info("=" * 60)
    logger.info("å¼€å§‹ç”Ÿæˆè¯è¡¨å’Œæ­£åˆ™è§„åˆ™")
    logger.info("=" * 60)
    logger.info(f"è¾“å…¥æ–‡ä»¶: {args.input}")
    logger.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    if args.samples_per_category is None:
        logger.info(f"é‡‡æ ·ç­–ç•¥: ä½¿ç”¨å…¨éƒ¨æ ·æœ¬")
    else:
        logger.info(f"æ¯ä¸ªç±»åˆ«é‡‡æ ·æ•°: {args.samples_per_category}")
    logger.info(f"ç”Ÿæˆç±»åˆ«: {args.categories}")
    logger.info(f"ç”Ÿæˆæ­£åˆ™è§„åˆ™: {args.generate_regex}")
    logger.info(f"APIè¯·æ±‚é—´éš”: {request_interval} ç§’ï¼ˆç¡®ä¿ä¸è¶…è¿‡30æ¬¡/åˆ†é’Ÿï¼‰")
    logger.info("")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return 1
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆè¯è¡¨
    all_words = {}
    all_patterns = []
    
    for category in args.categories:
        logger.info("")
        logger.info("-" * 60)
        logger.info(f"å¤„ç†ç±»åˆ«: {category}")
        logger.info("-" * 60)
        
        try:
            # 1. åŠ è½½æ ·æœ¬
            samples = load_data_samples(
                args.input,
                category,
                samples_per_category=args.samples_per_category
            )
            
            if len(samples) == 0:
                logger.warning(f"âš  {category} ç±»åˆ«æ²¡æœ‰æ‰¾åˆ°æ ·æœ¬ï¼Œè·³è¿‡")
                continue
            
            # 2. ç”Ÿæˆè¯è¡¨ï¼ˆåŠ¨æ€è¿½åŠ æ¨¡å¼ï¼Œé»˜è®¤ä½¿ç”¨å¼‚æ­¥ï¼‰
            output_path = os.path.join(args.output_dir, f"{category}.txt")
            words = generate_lexicon_from_samples(
                samples, 
                category,
                request_interval=request_interval,
                output_path=output_path,  # ä¼ å…¥è¾“å‡ºè·¯å¾„ï¼Œå¯ç”¨åŠ¨æ€è¿½åŠ 
                use_async=True  # ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
            )
            all_words[category] = words
            
            # 3. è¯è¡¨å·²åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿å­˜ï¼Œè¿™é‡Œåªè®°å½•æ—¥å¿—
            logger.info(f"âœ“ {category} è¯è¡¨å·²ä¿å­˜åˆ°: {output_path}ï¼ˆå…± {len(words)} ä¸ªè¯ï¼‰")
            
            # 4. ç”Ÿæˆæ­£åˆ™è§„åˆ™ï¼ˆå¯é€‰ï¼ŒåŠ¨æ€è¿½åŠ æ¨¡å¼ï¼‰
            if args.generate_regex:
                regex_path = os.path.join(args.output_dir, "regex_patterns.txt")
                patterns = generate_regex_patterns(
                    words, 
                    category,
                    request_interval=request_interval,
                    sample_words_count=20,
                    output_path=regex_path  # ä¼ å…¥è¾“å‡ºè·¯å¾„ï¼Œå¯ç”¨åŠ¨æ€è¿½åŠ 
                )
                all_patterns.extend(patterns)
            
            logger.info(f"âœ“ {category} ç±»åˆ«å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âœ— å¤„ç† {category} ç±»åˆ«æ—¶å‡ºé”™: {e}", exc_info=True)
            continue
    
    # æ•´ç†æ­£åˆ™è§„åˆ™æ–‡ä»¶ï¼ˆå¦‚æœç”Ÿæˆï¼Œå·²åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿½åŠ ï¼Œè¿™é‡Œåªéœ€å»é‡æ•´ç†ï¼‰
    if args.generate_regex and all_patterns:
        logger.info("")
        logger.info("-" * 60)
        logger.info("æ•´ç†æ­£åˆ™è§„åˆ™æ–‡ä»¶")
        logger.info("-" * 60)
        regex_path = os.path.join(args.output_dir, "regex_patterns.txt")
        if os.path.exists(regex_path):
            # è¯»å–æ‰€æœ‰è§„åˆ™ï¼Œå»é‡å¹¶æ’åº
            with open(regex_path, 'r', encoding='utf-8') as f:
                all_patterns = sorted(list(set(line.strip() for line in f if line.strip())))
            # é‡æ–°å†™å…¥ï¼ˆè¦†ç›–ï¼‰
            with open(regex_path, 'w', encoding='utf-8') as f:
                for pattern in all_patterns:
                    f.write(pattern + '\n')
            logger.info(f"âœ“ æ­£åˆ™è§„åˆ™æ–‡ä»¶å·²æ•´ç†ï¼Œå…± {len(all_patterns)} ä¸ªè§„åˆ™")
    
    # æ€»ç»“
    logger.info("")
    logger.info("=" * 60)
    logger.info("ç”Ÿæˆå®Œæˆ")
    logger.info("=" * 60)
    logger.info(f"ç”Ÿæˆçš„è¯è¡¨æ–‡ä»¶:")
    for category in args.categories:
        if category in all_words:
            count = len(all_words[category])
            file_path = os.path.join(args.output_dir, f"{category}.txt")
            logger.info(f"  - {file_path} ({count} ä¸ªè¯)")
    
    if args.generate_regex and all_patterns:
        regex_path = os.path.join(args.output_dir, "regex_patterns.txt")
        logger.info(f"  - {regex_path} ({len(all_patterns)} ä¸ªè§„åˆ™)")
    
    logger.info("")
    logger.info("æç¤ºï¼šç”Ÿæˆåè¯·äººå·¥å®¡æ ¸å’Œä¼˜åŒ–è¯è¡¨ï¼Œåˆ é™¤è¯¯æŠ¥ï¼Œè¡¥å……é—æ¼")
    
    return 0


if __name__ == '__main__':
    sys.exit(main())

